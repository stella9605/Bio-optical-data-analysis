run profile1

import math
train = pd.read_csv('train.csv', index_col=0)
test = pd.read_csv('test.csv', index_col=0)

train.head()

train.isnull().sum()[train.isnull().sum().values > 0]

# 결측치 보완
train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.
test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.
test_dst.head(1)

train_dst = train_dst.interpolate(methods='quadratic', axis=1)
test_dst = test_dst.interpolate(methods='quadratic', axis=1)

# 스팩트럼 데이터에서 보간이 되지 않은 값은 0으로 일괄 처리한다.
train_dst.fillna(0, inplace=True) 
test_dst.fillna(0, inplace=True)

test_dst.head(1)

train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.
test.update(test_dst)

# 산소포화도 = 옥시헤모글로빈 / (옥시헤모글로빈+디옥시헤모글로빈) * 100%



X = train.iloc[:, 0:-4]
train['SO'] = train.hhb / (train.hbo2 + train.hhb) * 100
X['SO'] = train['SO']
Y = train.iloc[:,-5:-1]

train_x, test_x, train_y, test_y = train_test_split(X,
                                                    Y,
                                                    random_state = 0)


# 각 조직에 관환 감쇠계수
attenuation_coefficient = pd.DataFrame({'NIR-1' : [10,48,25], 'NIR-2' : [7.4,45,23],'SWIR' : [6.5,44,21],'SWIR-2' : [9,49,24.5]})
attenuation_coefficient.index = ['Brain_cortex','Cranial_bone','Skin']
attenuation_coefficient.sum(axis = 0)

n =  attenuation_coefficient.sum(axis = 0)['NIR-2'] # 감쇠계수

## 튜닝 함수
def tuning_var(s):
    s_rho = s[0]          # _rho
    s_src = s[1:36]       # _src
    s_dst = s[36:-1]        # _dst
    s_SO = s[-1] # SO 산소포화도
    ss_SO = Series(s_SO)

    # index 표준화
    set_index = s_src.index.str.split('_').str[0]
    s_src.index = set_index
    s_dst.index = set_index

    # 계산식 (lambert beer 법칙)
    # A(흡광도) = -log10(I(투과방사선)/I0(입사방사선))  
    #           = ε(흡광계수) ⋅ b(투과 경로 길이(cm)) ⋅ c(농도)
    
    # 투광도
    transmittance = (s_dst / s_src)
    
    # 계산 완료후 inf,nan 0으로 치환
    transmittance = [i if i != np.inf else 0.0 for i in transmittance ]
    transmittance = Series(transmittance).fillna(value = 0)

    # math.log 계산을 위해 0을 1로 치환후 계산(흡광계수는 1로 가정한다.)
    transmittance = Series([1 if i == 0 else i for i in transmittance ])
    
    #흡광도_1 : -log10(I(투과방사선)/I0(입사방사선))  
    absorbance_1 = Series(map(lambda x : -math.log(x,10),transmittance))
    
    #흡광도_2 :  ε(흡광계수) ⋅ b(투과 경로 길이(cm)) ⋅ c(농도) (농도는 1로 가정)
    # 흡광계수는 3번째 논문에서 찾은 적외선 종류중 NIR-1 의 뇌피질, 두개골, 피부의 감쇠계수의 합을 적용시켰습니다.
    absorbance_2 = Series(1/(((((s_rho)/10)**2)*2) * (n * 2.01)))
    
    # 흡광도 index 설정
    absorbance_1.index = set_index.map(lambda x : 'A1_' + x)
    absorbance_2.index = ['A2_rho']
    ss_SO.index = ['SO']
    
    # 두 Series의 병합
    out_s = Series()
    out_s = out_s.append(absorbance_2).append(absorbance_1)
    out_s = out_s.append(ss_SO)
    # 튜닝된 설명변수의 Series반환
    return(out_s)

# train 변수 튜닝
tunning_train_x = train_x.apply(tuning_var, axis = 1)
tunning_test_x = test_x.apply(tuning_var, axis = 1)

#  제외 ----------------------------------------------------

# 릿지
m_ridge = Ridge(alpha=10)
m_ridge.fit(tunning_train_x, train_y)
m_ridge.fit(tunning_test_x, train_y)
m_ridge.score(tunning_train_x, train_y) # 76.7
m_ridge.score(tunning_test_x, test_y)  # 62.6

# 라쏘
m_lasso = Lasso(alpha=0.1)
m_lasso.fit(tunning_train_x, train_y)
m_lasso.score(train_x, train_y) # 75.9
m_lasso.score(test_x, test_y) # 60.7


# ---------------------------------------
# standard 스케일링
# minmax 스케일링
# 스케일링
m_sacled = StandardScaler()
m_sacled.fit(tunning_train_x)
x_scaled = m_sacled.transform(tunning_train_x)


# test_x 튜닝변수 스케일링
test_x_scaled = m_sacled.transform(tunning_test_x)

# ------------------------------------------
# minmax 스케일링

m_mms = MinMaxScaler()
m_mms.fit(tunning_train_x) # 각 설명변수의 최대, 최소 확인만
train_x_scaled = m_mms.transform(tunning_train_x) # 최대, 최소에 맞게 데이터 변환
test_x_scaled = m_mms.transform(tunning_test_x)

# -------------------------------------------------------
# robustScaler 스케일러


from sklearn.preprocessing import RobustScaler
m_rsc = RobustScaler()
m_rsc.fit(tunning_train_x)
x_train_robust_scale = m_rsc.transform(tunning_train_x)
x_test_robust_scale = m_rsc.transform(tunning_test_x)

# ------------------------------------------------------
# MaxAbsScaler 스케일러


from sklearn.preprocessing import MaxAbsScaler
maxAbsScaler = MaxAbsScaler()
maxAbsScaler.fit(tunning_train_x)
train_data_maxAbsScaled = maxAbsScaler.transform(tunning_train_x)
test_data_maxAbsScaled = maxAbsScaler.transform(tunning_test_x)



# 모델의 설정
model = Sequential()
model.add(Dense(17, input_dim=37, activation='relu')) 
model.add(Dense(4, activation='relu'))
 
# 모델 컴파일  
model.compile(loss='mean_absolute_error',
              optimizer='adam',
              metrics=['mae']) 
 
# 모델 실행 
model.fit(x_scaled, train_y, epochs=200, batch_size=10)  # standard
model.fit(train_x_scaled, train_y, epochs=200, batch_size=10)  # minmax
model.fit(x_train_robust_scale, train_y, epochs=200, batch_size=10) 
model.fit(x_scaled, train_y, epochs=200, batch_size=10) 




print("\n Accuracy: %.4f" % (model.evaluate(x_scaled,train_y)[1])) # standard 스케일링 1.3432     # 산소포화도 추가 1.1276
print("\n Accuracy: %.4f" % (model.evaluate(test_x_scaled,test_y)[1])) # standard 스케일링 1.4016  # 산소포화도 추가 1.1808

print("\n Accuracy: %.4f" % (model.evaluate(train_x_scaled,train_y)[1])) # minmax 스케일링 1.4244
print("\n Accuracy: %.4f" % (model.evaluate(test_x_scaled,test_y)[1])) # minmax 스케일링 1.4395

print("\n Accuracy: %.4f" % (model.evaluate(x_train_robust_scale,train_y)[1])) # robust 1.4929 
print("\n Accuracy: %.4f" % (model.evaluate(x_test_robust_scale,test_y)[1])) # robust 1.5370

print("\n Accuracy: %.4f" % (model.evaluate(train_data_maxAbsScaled,train_y)[1])) # maxabs
print("\n Accuracy: %.4f" % (model.evaluate(test_data_maxAbsScaled,test_y)[1])) # maxabs 

from keras.models import load_model
model.save('my_model.h5')   # h5 확장자로 저장

model2 = load_model('my_model.h5')
model2.evaluate(test_x_scaled,test_y)[1]  # 1.5639883279800415, 1.1717239618301392
model2.predict(test_x_scaled)
model2.predict(test_x_scaled)
test_x_scaled.shape

test_x_scaled[-1]
scaler = StandardScaler()
scaler.inverse_transform(test_x_scaled[-1])

X2 = X
X2.values = 
test1=pd.read_csv('sample_submission.csv',index_col='id')
test1=test1.astype('float')

# 진짜 테스트 변수 가공
tunning_realtest_x = test.apply(tuning_var, axis = 1)
realtest_x_scaled = m_sacled.transform(tunning_realtest_x)

test1.iloc[:,:]=model2.predict(realtest_x_scaled)
test1.to_csv("test1.csv")
