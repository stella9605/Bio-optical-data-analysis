##### *35개 반사도 컬럼 + 3개의 Y값 => 1개의 Y값 예측하기
#### 전처리
```python
run profile1
import math
train = pd.read_csv('train.csv', index_col=0)
test = pd.read_csv('test.csv', index_col=0)

train.isnull().sum()[train.isnull().sum().values > 0]

###### 결측치 보완
train_src = train.filter(regex='_src$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.
test_src = test.filter(regex='_src$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.
train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.
test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.
test_dst.head(1)

train_dst = train_dst.interpolate(methods='quadratic', axis=1)
test_dst = test_dst.interpolate(methods='quadratic', axis=1)
train_src = train_src.interpolate(methods='quadratic', axis=1)
test_src = test_src.interpolate(methods='quadratic', axis=1)


# 스팩트럼 데이터에서 보간이 되지 않은 값은 'bfill'로 일괄 처리한다.
train_dst=train_dst.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
test_dst=test_dst.apply(lambda x : x.fillna(method='bfill') ,axis=1) 

train_src=train_src.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
test_src=test_src.apply(lambda x : x.fillna(method='bfill') ,axis=1) 


train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.
test.update(test_dst)
train.update(train_src) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.
test.update(test_src)

X = train.iloc[:, :-4]
Y = train.iloc[:,-4:]

train_x, test_x, train_y, test_y = train_test_split(X,
                                                    Y,
                                                    random_state = 0)
```
#### 튜닝
```python
## 튜닝 함수
def tuning_var_R(s):
    s_rho = s[0]          # _rho
    s_src = s[1:36]       # _src
    s_dst = s[36:]        # _dst    

    # index 표준화
    set_index = s_src.index.str.split('_').str[0]
    s_src.index = set_index
    s_dst.index = set_index
    
    # 논문을 통해 빛의 이동경로를 고려한 빛의 세기는 다음과 같다
    # s_src * T1* R * T2 = s_dst
 
    # 반사도
    T1 = 3.9 * 10**(-4) * (10/4) 
    T2 = 3.9 * 10**(-4) * (10/4) 

    # 다음 두가지를 통해 모델링해본 결과 비슷함
    reflectance = s_dst / (s_src * (T1**2) * ((s_rho/10) * 2))
  # reflectance = s_dst / (s_src * (T1**2) * (s_rho/10)**2 * 2)

    reflectance = Series(map(lambda x : math.log(x,10), reflectance))

    # 흡광도 index 설정
    reflectance.index = set_index.map(lambda x : 'R_' + x)
    
    # 튜닝된 설명변수의 Series반환
    return(reflectance)


# train 변수 튜닝
tunning_train_x = train_x.apply(tuning_var_R, axis = 1)
tunning_test_x = test_x.apply(tuning_var_R, axis = 1)
```
#### 4개 모델에 대한 train 셋 만들기
```python
# Y3 뺀 설명변수 셋 만들기 <= Y3를 예측하기 위한 설명변수 셋임
m_sacled = StandardScaler()
m_sacled.fit(tunning_train_x)
m_sacled2 = StandardScaler()
m_sacled2.fit(train_y)

train_x_scaled = m_sacled.transform(tunning_train_x)
test_x_scaled = m_sacled.transform(tunning_test_x)
train_y_scaled = m_sacled2.transform(train_y)
test_y_scaled = m_sacled2.transform(test_y)

# 나머지 Y1 Y2 Y4에 대해서도 만들기
train_newY1=np.concatenate((train_x_scaled, train_y_scaled[:,[1,2,3]]), axis = 1)
test_newY1=np.concatenate((test_x_scaled, test_y_scaled[:,[1,2,3]]), axis = 1)
train_newY2=np.concatenate((train_x_scaled, train_y_scaled[:,[0,2,3]]), axis = 1)
test_newY2=np.concatenate((test_x_scaled, test_y_scaled[:,[0,2,3]]), axis = 1)
train_newY3=np.concatenate((train_x_scaled, train_y_scaled[:,[0,1,3]]), axis = 1)
test_newY3=np.concatenate((test_x_scaled, test_y_scaled[:,[0,1,3]]), axis = 1)
train_newY4=np.concatenate((train_x_scaled, train_y_scaled[:,[0,1,2]]), axis = 1)
test_newY4=np.concatenate((test_x_scaled, test_y_scaled[:,[0,1,2]]), axis = 1)
```
----------------------------------------
#### * 변수 정리
train, test : 전처리만 된 셋, 아직 컬럼은 71개 
train_x, test_x, train_y, test_y : 위에껄 train,test로 나눈거
tunning_train_x, tunning_test_x : 위에껄  tuning_var_R를 이용해 튜닝함, 35개 컬럼
train_x_scaled,test_x_scaled,train_y_scaled,test_y_scaled : 각각 스켈링한거
train_newY3 : Y3를 제외한 최종 train 데이터
-------------
#### 각 모델 4개에 대한 모델링
```python
1)
# 모델의 설정
model = Sequential() 
model.add(Dense(18, input_dim=38, activation='relu')) 
model.add(Dense(8, activation='relu'))
# model.add(Dense(4, activation='relu'))
model.add(Dense(1))
 
# 모델 컴파일  
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['MAE']) 
# 모델 실행 
model.fit(train_newY1, train_y.iloc[:,0], epochs=200, batch_size=100) 

print("\n MAE: %.4f" % (model.evaluate(train_newY1,train_y.iloc[:,0])[1])) # 1.13
print("\n MAE: %.4f" % (model.evaluate(test_newY1,test_y.iloc[:,0])[1])) # 1.14

# 모델 저장 
model.save('model_ann_bio_616Y1_h5')   # h5 확장자로 저장
model616Y1=load_model('model_ann_bio_616Y1_h5')


2)
# 모델의 설정
model = Sequential() 
model.add(Dense(18, input_dim=38, activation='relu')) 
model.add(Dense(8, activation='relu'))
# model.add(Dense(4, activation='relu'))
model.add(Dense(1))
 
# 모델 컴파일  
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['MAE']) 
# 모델 실행 
model.fit(train_newY2, train_y.iloc[:,1], epochs=200, batch_size=100) 

print("\n MAE: %.4f" % (model.evaluate(train_newY2,train_y.iloc[:,1])[1])) # 0.29
print("\n MAE: %.4f" % (model.evaluate(test_newY2,test_y.iloc[:,1])[1])) # 0.31

# 모델 저장 
model.save('model_ann_bio_616Y2_h5')   # h5 확장자로 저장
model616Y2=load_model('model_ann_bio_616Y2_h5')


3)
# 모델의 설정
model = Sequential() 
model.add(Dense(18, input_dim=38, activation='relu')) 
model.add(Dense(8, activation='relu'))
# model.add(Dense(4, activation='relu'))
model.add(Dense(1))
 
# 모델 컴파일  
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['MAE']) 
# 모델 실행 
model.fit(train_newY3, train_y.iloc[:,2], epochs=200, batch_size=100) 

print("\n MAE: %.4f" % (model.evaluate(train_newY3,train_y.iloc[:,2])[1])) # 1.27
print("\n MAE: %.4f" % (model.evaluate(test_newY3,test_y.iloc[:,2])[1])) # 1.3

# 모델 저장 
model.save('model_ann_bio_616Y3_h5')   # h5 확장자로 저장
model616Y3=load_model('model_ann_bio_616Y3_h5')
 

4)
# 모델의 설정
model = Sequential() 
model.add(Dense(18, input_dim=38, activation='relu')) 
model.add(Dense(8, activation='relu'))
# model.add(Dense(4, activation='relu'))
model.add(Dense(1))
 
# 모델 컴파일  
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['MAE']) 
# 모델 실행 
model.fit(train_newY4, train_y.iloc[:,3], epochs=200, batch_size=100) 

print("\n MAE: %.4f" % (model.evaluate(train_newY4,train_y.iloc[:,3])[1])) # 1.16
print("\n MAE: %.4f" % (model.evaluate(test_newY4,test_y.iloc[:,3])[1])) # 1.23

# 모델 저장 
model.save('model_ann_bio_616Y4_h5')   # h5 확장자로 저장
model616Y4=load_model('model_ann_bio_616Y4_h5')
```
#### test 셋에 적용해 최종 예측
```python
# 병합된 rtest set 만들기
rtest : 진짜 test 셋에 있는 설명변수의 조작된 35개 반사도 컬럼 + 가상의 3개 Y값을 가지는 테스트 set

# rtest_new 만들기
vtest615=pd.read_csv('test615.csv',index_col=0) # 어제 내가 예측한 Y값들임 이것을 가상의 Y값으로서 test set에 넣을거임

tunning_test=test.apply(tuning_var_R, axis = 1)
test_scaled = m_sacled.transform(tunning_test)
# m_sacled2.fit(vtest615.iloc[:,[0,1,3]])
m_sacled2.fit(vtest615) # 위 식이랑 무엇이 더 좋은지 봐야함
vtest615_scaled=m_sacled2.transform(vtest615)

# 최종 test 데이터 셋
rtest_newY1=np.concatenate((test_scaled,vtest615_scaled[:,[1,2,3]]), axis = 1)
rtest_newY2=np.concatenate((test_scaled,vtest615_scaled[:,[0,2,3]]), axis = 1)
rtest_newY3=np.concatenate((test_scaled,vtest615_scaled[:,[0,1,3]]), axis = 1)
rtest_newY4=np.concatenate((test_scaled,vtest615_scaled[:,[0,1,2]]), axis = 1)

# 모델에 적용해 최종 Y값 얻기
Y1=model616Y1.predict(rtest_newY1)
Y2=model616Y2.predict(rtest_newY2)
Y3=model616Y3.predict(rtest_newY3)
Y4=model616Y4.predict(rtest_newY4)

a=np.append(Y1,Y2,axis=1)
b=np.append(Y3,Y4,axis=1)
c=np.append(a,b,axis=1)

TEST=pd.read_csv('sample_submission.csv',index_col='id')
TEST.iloc[:,:]=c
TEST.to_csv("test616_1.csv")

