##### * 변수 저장
- 모델이나 변수를 저장할 목적으로 사용하면 될것같습니다. 파이썬 껏다키면 사라져서..
```python
import pickle

with open('612.p', 'wb') as file:    # james.p 파일을 바이너리 쓰기 모드(wb)로 열기
    for i in dir():
        pickle.dump(i.replace("'",""),file)                 
        
with open('612.p', 'rb') as file:    # james.p 파일을 바이너리 읽기 모드(rb)로 열기
    pickle.load(file)
    print(tunning_realtest_x)
```    
----------------
----------------
#### * 설명변수 대폭 축소해보기
- 4개의 모델에서 각각 예측하려는 하나의 Y값을 설명변수(모든 파장에 대한 흡광도 컬럼 35개 + 나머지 3개의 종속변수 농도값)
  로 예측하려고 했음
- 그런데 생각해보니 나머지 3개의 농도(ex. hbo2,na,ca)와 한 파장에서의 흡광도 값만으로도 Y(ex. hhb)값을 예측할 수 있을 거 같음
  이유 : 특정 파장에서의 흡광도 값은 4가지 물질의 농도가 정해지면 구할 수 있는데 구지 모든 파장으로부터 예측하려 한다면 모델이 복잡해질
         거 같았음        
- 따라서 설명변수로서 특정 파장컬럼 1개 + 나머지 종속변수3개로 설정하고 전체적인 모델링은 이전과 같이 4개모델로서 예측
- 0615_1과 겹치는 코드 일부 생략
- 1개의 파장컬럼은 어느것이 좋은 예측력을 보일지 모르니 각각 모두 선택해서 돌려봄
- 즉, 예를들어 hhb를 예측하는 모델1에 대해서
  650nm + hbo2,na,ca가 설명변수인 모델1_0이라면 660nm + hbo2,na,ca 설명변수인 모델1_2, ...  990nm + hbo2,na,ca가 설명변수인 모델1_34
  이것을 다 만들어서 좋은 예측을 보이는 모델1_x를 선정하는 것이 목적
- 그런식으로 가장 좋은 예측을 가지는 모델1_x1,모델2_x2,모델3_x3,모델4_x4을 각각 선정할 수 있으며 이것을 최종모델로 해서 hhb,hbo2,ca,na 를 각각 예측할려고 함 

##### * 결론
- MAE 0.1정도로 조금이나마 줄어듬
```python
tunning_train_edit1=tunning_train.iloc[:,1:]
tunning_test_edit1=tunning_test.iloc[:,1:] 

X1 = tunning_train_edit1.drop('hhb',axis=1)
Y1 = DataFrame(tunning_train_edit1.iloc[:,-4])
X2 = tunning_train_edit1.drop('hbo2',axis=1)
Y2 = DataFrame(tunning_train_edit1.iloc[:,-3])
X3 = tunning_train_edit1.drop('ca',axis=1)
Y3 = DataFrame(tunning_train_edit1.iloc[:,-2])
X4 = tunning_train_edit1.drop('na',axis=1)
Y4 = DataFrame(tunning_train_edit1.iloc[:,-1])


# 모델1,2,3,4에 대한 train,test data set을 나눔, seed값이 같으므로 각각 같은 train,test set이 만들어진다!!
train_x1_edit1, test_x1_edit1, train_y1_edit1, test_y1_edit1 = train_test_split(X1,
                                                    Y1,
                                                    random_state = 0)
train_x2_edit1, test_x2_edit1, train_y2_edit1, test_y2_edit1 = train_test_split(X2,
                                                    Y2,
                                                    random_state = 0)
train_x3_edit1, test_x3_edit1, train_y3_edit1, test_y3_edit1 = train_test_split(X3,
                                                    Y3,
                                                    random_state = 0)
train_x4_edit1, test_x4_edit1, train_y4_edit1, test_y4_edit1 = train_test_split(X4,
                                                    Y4,
                                                    random_state = 0)


## 모델1,2,3,4에 대해 각각 _0 ~ _34까지 모델을 만든다, 
# train_x1_0 ~ x1_34 설정하기
for i in range(0,35):
    exec('train_x1_'+ str(i) + '=' + 'train_x1_edit1.iloc[:,[i,-3,-2,-1]]')
    exec('test_x1_'+ str(i) + '=' + 'test_x1_edit1.iloc[:,[i,-3,-2,-1]]')

# train_x2_0 ~ x2_34 설정하기
for i in range(0,35):
    exec('train_x2_'+ str(i) + '=' + 'train_x2_edit1.iloc[:,[i,-3,-2,-1]]')
    exec('test_x2_'+ str(i) + '=' + 'test_x2_edit1.iloc[:,[i,-3,-2,-1]]')

# train_x3_0 ~ x3_34 설정하기
for i in range(0,35):
    exec('train_x3_'+ str(i) + '=' + 'train_x3_edit1.iloc[:,[i,-3,-2,-1]]')
# train_x4_0 ~ x4_34 설정하기
for i in range(0,35):
    exec('train_x4_'+ str(i) + '=' + 'train_x4_edit1.iloc[:,[i,-3,-2,-1]]')
train_x3_0
train_x3_1 

# 모델의 설정, input_dim 주의, 아래 세 개의 모델프레임 중 첫번째 거로 진행함, 최적화프레임은 나중에 선정할 것
    
model = Sequential() 
model.add(Dense(18, input_dim=6, activation='relu')) 
model.add(Dense(8, activation='relu'))
model.add(Dense(1))
 
model = Sequential() 
model.add(Dense(4, input_dim=4, activation='relu')) 
model.add(Dense(1))

model = Sequential() 
model.add(Dense(8, input_dim=4, activation='relu')) 
model.add(Dense(1))
# 모델 컴파일  
model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['MAE']) 


*1)
# 모델1_0 ~ 모델1_34까지 돌려서 train, test set에 대한 결과값을  v_trainY1, v_testY 이름의 리스트에 넣음
 - 결과 : 파장 컬럼이 830, 840, 850 nm로 설정했을 때 가장 높은 예측력을 보임 

v_trainY1=[]
v_testY1=[]
for i in range(0,35):
    # 모델 실행 
    exec('model.fit(train_x1_'+str(i)+', train_y1_edit1, epochs=200, batch_size=60)') 

    exec('a'+'='+'model.evaluate(train_x1_'+str(i)+',train_y1_edit1)[1]') 
    exec('b'+'='+'model.evaluate(test_x1_'+str(i)+',test_y1_edit1)[1]')
    v_trainY1.append(a) 
    v_testY1.append(b)

s_trainY1=Series(v_trainY1,index=tunning_train_edit1.columns[:-4])
s_testY1=Series(v_testY1,index=tunning_train_edit1.columns[:-4])
s_trainY1
s_testY1
830~850


- 각 모델1,2,3,4에서 선정된 높은 예측력을 갖는 3개 정도의 파장값들을 모두 고려하기로 함
- 즉 설명변수로서 특정 파장컬럼 3개 + 나머지 종속변수3개로 설정해서 최종 모델링ㄱㄱ


*2)
# 모델2_0 ~ 모델2_34까지 돌려서 train, test set에 대한 결과값을  v_trainY1, v_testY 이름의 리스트에 넣음
 - 결과 : 파장 컬럼이 810, 820, 830 nm로 설정했을 때 가장 높은 예측력을 보임 

v_trainY2=[]
v_testY2=[]
for i in range(0,35):
    # 모델 실행 
    exec('model.fit(train_x2_'+str(i)+', train_y2_edit1, epochs=200, batch_size=60)') 

    exec('a'+'='+'model.evaluate(train_x2_'+str(i)+',train_y2_edit1)[1]') 
    exec('b'+'='+'model.evaluate(test_x2_'+str(i)+',test_y2_edit1)[1]')
    v_trainY2.append(a) 
    v_testY2.append(b)

s_trainY2=Series(v_trainY2,index=tunning_train_edit1.columns[:-4])
s_testY2=Series(v_testY2,index=tunning_train_edit1.columns[:-4])
s_trainY2
s_testY2
810~830


*3)
 - 결과 : 파장 컬럼이 840, 850, 860 nm로 설정했을 때 가장 높은 예측력을 보임  

v_trainY3=[]
v_testY3=[]
for i in range(0,35):
    # 모델 실행 
    exec('model.fit(train_x3_'+str(i)+', train_y3_edit1, epochs=200, batch_size=40)') 

    exec('a'+'='+'model.evaluate(train_x3_'+str(i)+',train_y3_edit1)[1]') 
    exec('b'+'='+'model.evaluate(test_x3_'+str(i)+',test_y3_edit1)[1]')
    v_trainY3.append(a) 
    v_testY3.append(b)



    # 모델 저장 
    model.save('model_ann_bio_612Y3_0_h5')   # h5 확장자로 저장
    model612Y3_0=load_model('model_ann_bio_612Y3_0_h5')

s_trainY3=Series(v_trainY3,index=tunning_train_edit1.columns[:-4])
s_testY3=Series(v_testY3,index=tunning_train_edit1.columns[:-4])
s_trainY3
s_testY3
840~860


*4)
 - 결과 : 파장 컬럼이 810, 820, 830 nm로 설정했을 때 가장 높은 예측력을 보임

v_trainY4=[]
v_testY4=[]
for i in range(0,35):
    # 모델 실행 
    exec('model.fit(train_x4_'+str(i)+', train_y4_edit1, epochs=200, batch_size=50)') 

    exec('a'+'='+'model.evaluate(train_x4_'+str(i)+',train_y4_edit1)[1]') 
    exec('b'+'='+'model.evaluate(test_x4_'+str(i)+',test_y4_edit1)[1]')
    v_trainY4.append(a) 
    v_testY4.append(b)



    # 모델 저장 
    model.save('model_ann_bio_612Y3_0_h5')   # h5 확장자로 저장
    model612Y3_0=load_model('model_ann_bio_612Y3_0_h5')

s_trainY4=Series(v_trainY4,index=tunning_train_edit1.columns[:-4])
s_testY4=Series(v_testY4,index=tunning_train_edit1.columns[:-4])

s_trainY4
s_testY4
810~830
```
-----------------------
-----------------------
- 위에서 나온 각 모델1,2,3,4에서 선정된 높은 예측력을 갖는 3개 정도의 파장값들을 모두 고려하기로 생각함
- 즉 설명변수로서 특정 파장컬럼 3개 + 나머지 종속변수3개로 설정해서 최종 모델링ㄱㄱ
```python
**1) 830~850 만 적용한 데이터셋으로 Y1만들기
train_x1_18.iloc[:,0]
train_x1_19.iloc[:,0]
train_x1_20
d=pd.merge(train_x1_18.iloc[:,0],train_x1_19.iloc[:,0],left_index=True, right_index=True)
train_x1_830850=pd.merge(d,train_x1_20,left_index=True, right_index=True)

test_x1_18.iloc[:,0]
test_x1_19.iloc[:,0]
test_x1_20
D=pd.merge(test_x1_18.iloc[:,0],test_x1_19.iloc[:,0],left_index=True, right_index=True)
test_x1_830850=pd.merge(D,test_x1_20,left_index=True, right_index=True)

# 모델 실행 
model.fit(train_x1_830850, train_y1_edit1, epochs=200, batch_size=70) 

print("\n MAE: %.4f" % (model.evaluate(train_x1_830850,train_y1_edit1)[1]))
print("\n MAE: %.4f" % (model.evaluate(test_x1_830850,test_y1_edit1)[1])) # 0.43

# 모델 저장 
model.save('model_ann_bio_612Y1_830850_h5')   # h5 확장자로 저장
model612Y1_830850=load_model('model_ann_bio_612Y1_830850_h5')


**2) 810~830 만 적용한 데이터셋으로 Y2만들기
train_x2_16.iloc[:,0]
train_x2_17.iloc[:,0]
train_x2_18
c=pd.merge(train_x2_16.iloc[:,0],train_x2_17.iloc[:,0],left_index=True, right_index=True)
train_x2_810830=pd.merge(c,train_x2_18,left_index=True, right_index=True)

test_x2_16.iloc[:,0]
test_x2_17.iloc[:,0]
test_x2_18
C=pd.merge(test_x2_16.iloc[:,0],test_x2_17.iloc[:,0],left_index=True, right_index=True)
test_x2_810830=pd.merge(C,test_x2_18,left_index=True, right_index=True)


# 모델 실행 
model.fit(train_x2_810830, train_y2_edit1, epochs=200, batch_size=70) 

print("\n MAE: %.4f" % (model.evaluate(train_x2_810830,train_y2_edit1)[1]))
print("\n MAE: %.4f" % (model.evaluate(test_x2_810830,test_y2_edit1)[1])) # 0.43

# 모델 저장 
model.save('model_ann_bio_612Y2_810830_h5')   # h5 확장자로 저장
model612Y2_810830=load_model('model_ann_bio_612Y2_810830_h5')


**3) 840~860 만 적용한 데이터셋으로 Y3만들기
train_x3_19.iloc[:,0]
train_x3_20.iloc[:,0]
train_x3_21
a=pd.merge(train_x3_19.iloc[:,0],train_x3_20.iloc[:,0],left_index=True, right_index=True)
train_x3_840860=pd.merge(a,train_x3_21,left_index=True, right_index=True)

test_x3_19.iloc[:,0]
test_x3_20.iloc[:,0]
test_x3_21
A=pd.merge(test_x3_19.iloc[:,0],test_x3_20.iloc[:,0],left_index=True, right_index=True)
test_x3_840860=pd.merge(A,test_x3_21,left_index=True, right_index=True)


# 모델 실행 
model.fit(train_x3_840860, train_y3_edit1, epochs=200, batch_size=70) 

print("\n MAE: %.4f" % (model.evaluate(train_x3_840860,train_y3_edit1)[1]))
print("\n MAE: %.4f" % (model.evaluate(test_x3_840860,test_y3_edit1)[1])) # 0.43

# 모델 저장 
model.save('model_ann_bio_612Y3_840860_h5')   # h5 확장자로 저장
model612Y3_840860=load_model('model_ann_bio_612Y3_840860_h5')


**4) 810~830 만 적용한 데이터셋으로 Y4만들기
train_x4_16.iloc[:,0]
train_x4_17.iloc[:,0]
train_x4_18
b=pd.merge(train_x4_16.iloc[:,0],train_x4_17.iloc[:,0],left_index=True, right_index=True)
train_x4_810830=pd.merge(b,train_x4_18,left_index=True, right_index=True)

test_x4_16.iloc[:,0]
test_x4_17.iloc[:,0]
test_x4_18
B=pd.merge(test_x4_16.iloc[:,0],test_x4_17.iloc[:,0],left_index=True, right_index=True)
test_x4_810830=pd.merge(B,test_x4_18,left_index=True, right_index=True)


# 모델 실행 
model.fit(train_x4_810830, train_y4_edit1, epochs=200, batch_size=70) 

print("\n MAE: %.4f" % (model.evaluate(train_x4_810830,train_y4_edit1)[1]))
print("\n MAE: %.4f" % (model.evaluate(test_x4_810830,test_y4_edit1)[1])) # 0.43

# 모델 저장 
model.save('model_ann_bio_612Y4_810830_h5')   # h5 확장자로 저장
model612Y4_810830=load_model('model_ann_bio_612Y4_810830_h5')
```

--------------------
##### *realtest에 적용
```python
# realtest도 train했던 방식과 동일하게 변수데이터 셋 설정
TX1=tunning_test_edit1.drop('hhb',axis=1).iloc[:,[18,19,20,-3,-2,-1]]
TX2=tunning_test_edit1.drop('hbo2',axis=1).iloc[:,[16,17,18,-3,-2,-1]]
TX3=tunning_test_edit1.drop('ca',axis=1).iloc[:,[19,20,21,-3,-2,-1]]
TX4=tunning_test_edit1.drop('na',axis=1).iloc[:,[16,17,18,-3,-2,-1]]

# 모델에 적용 밒 평가
Y1=model612Y1_830850.predict(TX1)
Y2=model612Y2_810830.predict(TX2)
Y3=model612Y3_840860.predict(TX3)
Y4=model612Y4_810830.predict(TX4)

a=np.append(Y1,Y2,axis=1)
b=np.append(Y3,Y4,axis=1)
c=np.append(a,b,axis=1)
d=m_sacled.inverse_transform(c)

test=pd.read_csv('sample_submission.csv',index_col='id')

test.iloc[:,:]=d
test.to_csv("test612_3.csv")