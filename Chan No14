#### train,test data set
run profile_project

train = pd.read_csv('train.csv', index_col=0)
test = pd.read_csv('test.csv', index_col=0)

################################################################ new 결측지 보완
# 결측치 보완
train_src = train.filter(regex='_src$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.
test_src = test.filter(regex='_src$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.
train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.
test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.
test_dst.head(1)

# na컬럼 인덱스
train_index_na=train_src.isna().values | train_dst.isna().values
test_index_na=test_src.isna().values | test_dst.isna().values

# 보간(형식적)
train_dst = train_dst.interpolate(methods='quadratic', axis=1)
test_dst = test_dst.interpolate(methods='quadratic', axis=1)
train_src = train_src.interpolate(methods='quadratic', axis=1)
test_src = test_src.interpolate(methods='quadratic', axis=1)
# 스팩트럼 데이터에서 보간이 되지 않은 값은 'bfill'로 일괄 처리한다.
train_dst=train_dst.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
test_dst=test_dst.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
train_src=train_src.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
test_src=test_src.apply(lambda x : x.fillna(method='bfill') ,axis=1) 

train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.
test.update(test_dst)
train.update(train_src) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.
test.update(test_src)

X = train.iloc[:, :-4]
Y = train.iloc[:,-4:]

# train 변수 튜닝
tunning_X = X.apply(tuning_var, axis = 1)
tunning_test = test.apply(tuning_var, axis = 1)

# 이제 반사도 컬럼으로 변환됐으니 na값을 가졌던 컬럼인덱스를 사용해 NaN을 넣어준다 
tunning_X.iloc[:,1:].values[train_index_na]=np.NaN
tunning_test.iloc[:,1:].values[test_index_na]=np.NaN

# A2_rho 백업
tunning_X_rho = tunning_X.iloc[:,0:1]
tunning_test_rho = tunning_test .iloc[:,0:1]

tunning_X = tunning_X.iloc[0:,1:]
tunning_test = tunning_test.iloc[0:,1:]

# 최종 보간
tunning_X = tunning_X.interpolate(methods='quadratic', axis=1)
tunning_test = tunning_test.interpolate(methods='quadratic', axis=1)

# 스팩트럼 데이터에서 보간이 되지 않은 값은 'bfill'로 일괄 처리한다.
tunning_X=tunning_X.apply(lambda x : x.fillna(method='bfill') ,axis=1) 
tunning_test=tunning_test.apply(lambda x : x.fillna(method='bfill') ,axis=1) 

# 보간된 데이터 프레임과 A2_rho 데이터 프레임 결합
tunning_X = pd.concat([tunning_X_rho, tunning_X ], axis = 1)
tunning_test = pd.concat([tunning_test_rho, tunning_test ], axis = 1)

# 스케일링
rs = RobustScaler() # 표준화 변환시에는 “이상치, 특이값 (outlier)이 없어야 한다”, 표준화 후 동일한 값을 더 넓게 분포
rs.fit(tunning_X)
tunning_X_scaled = rs.transform(tunning_X)
tunning_test_scaled = rs.transform(tunning_test)

# train, test set
train_x, test_x, train_y, test_y = train_test_split(tunning_X_scaled,
                                                    Y,
                                                    random_state = 0)



## new 결측치 보완은 튜닝 전,후를 다 보완했기 때문에 이미 튜닝이 되있는 상태입니다.
##################################################매개 변수 search
n_estimators = [500,800,1500,2500,5000]
max_features = ['auto', 'sqrt', 'log2']
max_depth = [10,20,30,40,50]
max_depth.append(None)
min_samples_split  = [2,5,10,15,20]
min_samples_leaf = [1,2,5,10,15]

##########  최적화
n_estimators_mae = []
for i in min_samples_split:
    m_rf_hbo2 = rf_r(min_samples_split = i)
    m_rf_hbo2.fit(train_x, list(train_y['ca']))
    
    test_y_hbo2_predict = m_rf_hbo2.predict(test_x)

    mae_v = mean_absolute_error(test_y['ca'], test_y_hbo2_predict)  
    
    n_estimators_mae.append(mae_v)
           
############################################## 모델 변경(RandomForestRegressor)
## hhb
m_rf_hhb = rf_r(n_estimators = 2000,
                max_features = 'auto',
                max_depth = 20,
                min_samples_split = 5,
                min_samples_leaf = 2,
                n_jobs = -1)
m_rf_hhb.fit(train_x, list(train_y['hhb']))

train_y_hhb_predict = m_rf_hhb.predict(train_x)
test_y_hhb_predict = m_rf_hhb.predict(test_x)

####MAE
mean_absolute_error(train_y['hhb'], train_y_hhb_predict)       # 0.326
mean_absolute_error(test_y['hhb'], test_y_hhb_predict)         # 0.774

## hbo2 
m_rf_hbo2 = rf_r(n_estimators = 2000,
                 max_features = 'auto',
                 max_depth = 30,
                 min_samples_split = 2,
                 min_samples_leaf = 2,
                 n_jobs = -1)
m_rf_hbo2.fit(train_x, list(train_y['hbo2']))

test_y_hbo2_predict = m_rf_hbo2.predict(test_x)
train_y_hbo2_predict = m_rf_hbo2.predict(train_x)

####MAE
mean_absolute_error(train_y['hbo2'], train_y_hbo2_predict)       # 0.230
mean_absolute_error(test_y['hbo2'], test_y_hbo2_predict)         # 0.573

## ca
m_rf_ca = rf_r(n_estimators = 2000,
               max_features = 'auto',
               max_depth = 40,
               min_samples_split = 5,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_ca.fit(train_x, list(train_y['ca']))

test_y_ca_predict = m_rf_ca.predict(test_x)
train_y_ca_predict = m_rf_ca.predict(train_x)

####MAE
mean_absolute_error(train_y['ca'], train_y_ca_predict)    # 0.751
mean_absolute_error(test_y['ca'], test_y_ca_predict)      # 1.850

## na
m_rf_na = rf_r(n_estimators = 2000,
               max_features = 'auto', 
               max_depth = 40,
               min_samples_split = 2,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_na.fit(train_x, list(train_y['na']))

test_y_na_predict = m_rf_na.predict(test_x)
train_y_na_predict = m_rf_na.predict(train_x)

####MAE
mean_absolute_error(train_y['na'], train_y_na_predict)    # 0.450
mean_absolute_error(test_y['na'], test_y_na_predict)      # 1.235

############################################################## train 전체 학습
## hhb
m_rf_hhb = rf_r(n_estimators = 2000,
                max_features = 'auto',
                max_depth = 20,
                min_samples_split = 5,
                min_samples_leaf = 2,
                n_jobs = -1)
m_rf_hhb.fit(tunning_X_scaled, list(Y['hhb']))

## hbo2 
m_rf_hbo2 = rf_r(n_estimators = 2000,
                 max_features = 'auto',
                 max_depth = 30,
                 min_samples_split = 2,
                 min_samples_leaf = 2,
                 n_jobs = -1)
m_rf_hbo2.fit(tunning_X_scaled, list(Y['hbo2']))

## ca
m_rf_ca = rf_r(n_estimators = 2000,
               max_features = 'auto',
               max_depth = 40,
               min_samples_split = 5,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_ca.fit(tunning_X_scaled, list(Y['ca']))

## na
m_rf_na = rf_r(n_estimators = 2000,
               max_features = 'auto', 
               max_depth = 40,
               min_samples_split = 2,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_na.fit(tunning_X_scaled, list(Y['na']))

#################### hhb + hbo2 예측 후 ca 예측

#hhb
tunning_x_hhb_predict = m_rf_hhb.predict(tunning_X_scaled)
tunning_test_hhb_predict = m_rf_hhb.predict(tunning_test_scaled)
#hbo2
tunning_x_hbo2_predict = m_rf_hbo2.predict(tunning_X_scaled)
tunning_test_hbo2_predict = m_rf_hbo2.predict(tunning_test_scaled)

#스케일링 전 tunning_X, tunning_test에 hhb, hbo2 추가
tunning_X_plus = pd.concat([tunning_X, 
                            pd.DataFrame(tunning_x_hhb_predict, columns = ['hhb']),
                            pd.DataFrame(tunning_x_hbo2_predict, columns = ['hbo2'])], 
                           axis = 1)

tunning_test_plus = pd.concat([tunning_test, 
                               pd.DataFrame(tunning_test_hhb_predict, index = tunning_test.index, columns = ['hhb']),
                               pd.DataFrame(tunning_test_hbo2_predict, index = tunning_test.index, columns = ['hbo2'])], 
                              axis = 1)

# 스케일링
rs = RobustScaler() # 표준화 변환시에는 “이상치, 특이값 (outlier)이 없어야 한다”, 표준화 후 동일한 값을 더 넓게 분포
rs.fit(tunning_X_plus)
tunning_X_plus_scaled = rs.transform(tunning_X_plus)
tunning_test_plus_scaled = rs.transform(tunning_test_plus)

train_x, test_x, train_y, test_y = train_test_split(tunning_X_plus_scaled,
                                                    Y,
                                                    random_state = 0)
# ca
m_rf_ca = rf_r(n_estimators = 2000,
               max_features = 'auto',
               max_depth = 40,
               min_samples_split = 5,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_ca.fit(train_x, list(train_y['ca']))

test_y_ca_predict = m_rf_ca.predict(test_x)
train_y_ca_predict = m_rf_ca.predict(train_x)

####MAE
mean_absolute_error(train_y['ca'], train_y_ca_predict)    # 0.659
mean_absolute_error(test_y['ca'], test_y_ca_predict)      # 1.625

## na
m_rf_na = rf_r(n_estimators = 2000,
               max_features = 'auto', 
               max_depth = 40,
               min_samples_split = 2,
               min_samples_leaf = 1,
               n_jobs = -1)
m_rf_na.fit(train_x, list(train_y['na']))

test_y_na_predict = m_rf_na.predict(test_x)
train_y_na_predict = m_rf_na.predict(train_x)

####MAE
mean_absolute_error(train_y['na'], train_y_na_predict)    # 0.449
mean_absolute_error(test_y['na'], test_y_na_predict)      # 1.232

# 기존 test.csv 제출 결과 : 1.100
# test.csv 예측후 제출 결과 mae 값 :1.113

## 결론: train.csv파일의 hhb, hbo2를 예측 후 컬럼으로 추가해 na,ca를 예측한 후 train을 분리해서
##      train과 test를 예측한 결과 평균 오차가 낮아지는 것을 확인했지만,
##      test.csv파일의  hhb, hbo2를 예측 후 컬럼으로 추가해 na,ca를 예측한 후 제출한결과 오히려
##      평균오차가 높아지는 것을 확인했습니다.

## 예상: hhb와 hbo2의 예측값의 오차가 있기 때문에, 이 예측값들을 가지고
##       na,ca의 예측값들이 실제보다 더 평균오차가 높아진다고 판단됩니다.


############################################################# TEST.csv 채워넣기
#### 제출 : 1.100  csv 파일
test2=pd.read_csv('test 2020-6-18.csv',index_col='id')

#### 철균씨의 NA set
df_na = pd.read_csv('na.csv')
df_na = df_na.set_index(['id'])
test2['na'] = df['na']

#### 주희씨의 hhb set
df_hhb = pd.read_csv('hhb.csv')
df_hhb = df_hhb.set_index(['id'])
test2['hhb'] = df_hhb['hhb']

### write test2.csv
test2.to_csv("test 2020-6-22(hhb_set).csv")

## sample_submission.csv load
test1=pd.read_csv('sample_submission.csv',index_col='id')
test1=test1.astype('float')

#### Y값 
test_hhb = m_rf_hhb.predict(tunning_test_scaled)
test_hbo2 = m_rf_hbo2.predict(tunning_test_scaled)
test_ca = m_rf_ca.predict(tunning_test_scaled)
test_na = m_rf_na.predict(tunning_test_scaled)

### test1 <- Y 예측값 삽입
test1['hhb'] = test_hhb
test1['hbo2'] = test_hbo2
test1['ca'] = test_ca
test1['na'] = test_na

########## RandomForestRegressor 모델
test1.to_csv("test 2020-6-22.csv")
